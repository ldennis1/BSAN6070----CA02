{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02--LaurenDennis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "18o2aCcTBdO6eUx1oHt9_CvENiKwVr8nr",
      "authorship_tag": "ABX9TyPRY0OxqktCsdMHFCpbX+5w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldennis1/BSAN6070--CA02/blob/main/CA02_LaurenDennis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective: Na√Øve Bayes Classifier to create a spam filtering email algorithm "
      ],
      "metadata": {
        "id": "9xIRLar4lM1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary packages \n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "gze7FVU2lEI7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''This function builds a Dictionary of most common 3000 words from all the email content. \n",
        "First it adds all words and symbols in the dictionary. \n",
        "Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. \n",
        "After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. \n",
        "It returns the Dictionary.'''\n",
        "\n",
        "def make_Dictionary(init_dir):   \n",
        "  total_words = [] #step to create dict of all words from email content \n",
        "  emails = [os.path.join(init_dir,f) for f in os.listdir(init_dir)] \n",
        "  for allmail in emails:\n",
        "    with open(allmail) as m:\n",
        "      for line in m:\n",
        "        words = line.split() #splitting sentences into lists \n",
        "        total_words += words\n",
        "  dictionary = Counter(total_words) #gives us the frequency of the words in the dictionary\n",
        "  list_to_remove = list(dictionary)\n",
        "\n",
        "  for charac in list_to_remove:\n",
        "    if charac.isalpha() == False: #removes non-alpha-numeric\n",
        "      del dictionary[charac]\n",
        "    elif len(charac) == 1: #removes single character alpha-numeric characters \n",
        "      del dictionary[charac]\n",
        "  dictionary = dictionary.most_common(3000) #most frequent 3000 words to the dictionary\n",
        "  return dictionary"
      ],
      "metadata": {
        "id": "i99ELQ-AmQ5p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This function extracts feature columns and populates their values \n",
        "(Feature Matrix of 3000 comumns and rows equal to the number of email files). \n",
        "The function also analyzes the File Names of each email file and decides if it's \n",
        "a Spam or not based on the naming convention. Based on this the function also \n",
        "creates the Labelled Data Column. This function is used to extract the training dataset \n",
        "as well as the testing dataset and returns the Feature Dataset and the Label column.\"\"\"\n",
        "\n",
        "def extract_features(mail_dir): \n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)] \n",
        "  features_matrix = np.zeros((len(files),3000)) #3000 columns and rows to correspond with initial directory of 3000\n",
        "                                                #most common words\n",
        "  train_labels = np.zeros(len(files)) \n",
        "  count = 1;\n",
        "  docID = 0;\n",
        "  for fil in files: #uses a for loop and enumerate to number the items in the dict \n",
        "    with open(fil) as fi:\n",
        "      for i, line in enumerate(fi):\n",
        "        if i ==2:\n",
        "          words = line.split()\n",
        "          for word in words:\n",
        "            wordID = 0\n",
        "            for i, d in enumerate(dictionary):\n",
        "              if d[0] == word:\n",
        "                wordID = i\n",
        "                features_matrix[docID,wordID] = words.count(word)\n",
        "      train_labels[docID] = 0;\n",
        "      filepathTokens = fil.split('/')\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
        "      if lastToken.startswith(\"spmsg\"): #iterates through file names to decide if spam/real based on the given naming legend\n",
        "        train_labels[docID] = 1;\n",
        "        count = count + 1\n",
        "      docID = docID + 1\n",
        "  return features_matrix, train_labels"
      ],
      "metadata": {
        "id": "5f1GFVBkqqv4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"The section is the main Program that calls the above two functions and gets executed first. \n",
        "First it \"trains\" the model using model.fit function and Training Dataset. \n",
        "After that it scores the Test Data set by running the Trained Model with the Test Data set. \n",
        "At the end it prints the model performance in terms of accuracy score.\"\"\"\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/6070 BRAHMA/CA02/Data/train-mails' #mounted from google drive \n",
        "test_dir = '/content/drive/MyDrive/6070 BRAHMA/CA02/Data/test-mails' #mounted from google drive \n",
        "\n",
        "dictionary = make_Dictionary(train_dir)\n",
        "\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(train_dir) #trains the model\n",
        "test_features_matrix, test_labels = extract_features(test_dir) #tests the trained model with the test data\n",
        "\n",
        "model = GaussianNB() #implements the Gaussian Naive Bayes classifier \n",
        "\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels) #passes the functions into the Gaussian Naive Bayes Classifier model \n",
        "print (\"Training completed\")\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(test_labels, predicted_labels)) #shows the calculation of predictions the model got correct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz1ftEtQq860",
        "outputId": "a5ba4c05-8fcf-4eac-b79e-f7397b30384d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ]
        }
      ]
    }
  ]
}